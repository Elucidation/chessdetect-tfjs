<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chessboard Finder</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/dist/tf-backend-webgpu.js"></script>

    <style>
        /* Custom styles */
        body { font-family: 'Press Start 2P', cursive; overscroll-behavior: none; touch-action: manipulation; }
        canvas, video { border: 1px solid #a0aec0; background-color: #f7fafc; max-width: 100%; height: auto; aspect-ratio: 1 / 1; display: block; margin: 0 auto; }
        .canvas-container { display: flex; flex-direction: column; align-items: center; margin-bottom: 1rem; width: 100%; background-color: rgba(255, 255, 255, 0.5); padding: 0.75rem; border-radius: 0.5rem; border: 1px solid #e2e8f0; }
        label { margin-top: 0.5rem; font-size: 0.75rem; color: #4a5568; text-align: center; line-height: 1.2; }
        button:active { transform: scale(0.95); }
        .loader { border: 4px solid #e0e7ff; border-top: 4px solid #4f46e5; border-radius: 50%; width: 16px; height: 16px; animation: spin 1s linear infinite; display: inline-block; margin-left: 8px; vertical-align: middle; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        input[type=range] { height: 20px; -webkit-appearance: none; margin: 10px 0; width: 80%; background: transparent; }
        input[type=range]:focus { outline: none; }
        input[type=range]::-webkit-slider-runnable-track { width: 100%; height: 6px; cursor: pointer; background: #7f9cf5; border-radius: 0px; border: 1px solid #4a5568; }
        input[type=range]::-webkit-slider-thumb { height: 18px; width: 18px; border-radius: 0px; background: #4f46e5; cursor: pointer; -webkit-appearance: none; margin-top: -7px; border: 1px solid #2d3748; }
        input[type=range]::-moz-range-track { width: 100%; height: 6px; cursor: pointer; background: #7f9cf5; border-radius: 0px; border: 1px solid #4a5568; }
        input[type=range]::-moz-range-thumb { height: 18px; width: 18px; border-radius: 0px; background: #4f46e5; cursor: pointer; border: 1px solid #2d3748; }
        /* Hide/Show */
        .hidden { display: none; }
        /* Style video element - keep size for JS but hide */
        #videoFeed { width: 256px; height: 256px; object-fit: cover; }
    </style>
</head>
<body class="bg-gradient-to-br from-indigo-100 via-purple-100 to-pink-100 flex items-center justify-center min-h-screen p-4">

    <div class="bg-white p-6 sm:p-8 rounded-xl shadow-2xl w-full max-w-4xl text-gray-800">
        <h1 class="text-2xl sm:text-3xl font-bold text-indigo-700 mb-6 text-center">Chessboard Finder</h1>

        <div class="text-center mb-4">
            <button id="modeFileButton" class="bg-purple-600 text-white font-semibold py-1 px-3 rounded-l-md text-xs focus:outline-none ring-2 ring-purple-600 z-10 relative">File Upload</button><button id="modeCameraButton" class="bg-gray-300 text-gray-700 font-semibold py-1 px-3 rounded-r-md text-xs focus:outline-none relative">Use Camera</button>
        </div>

        <div id="fileInputSection" class="mb-6 grid grid-cols-1 md:grid-cols-3 gap-4 items-center px-4">
             <div class="text-center md:text-left">
                <label class="block text-xs font-medium text-gray-600 mb-1 uppercase tracking-wider">Model Status</label>
                <div id="modelStatus" class="text-sm text-indigo-700 p-2 border border-indigo-300 rounded-md bg-indigo-50 h-[40px] flex items-center justify-center md:justify-start">Loading...</div>
            </div>
            <div class="text-center md:text-left">
                <label for="imageInput" class="block text-xs font-medium text-gray-600 mb-1 uppercase tracking-wider">Upload Image</label>
                <input type="file" id="imageInput" accept="image/*" class="block w-full text-sm text-gray-700 border border-gray-400 rounded-md cursor-pointer
                    file:mr-3 file:py-2 file:px-4 file:border-0 file:text-sm file:font-semibold
                    file:bg-indigo-100 file:text-indigo-700 hover:file:bg-indigo-200 file:cursor-pointer" disabled>
            </div>
            <div class="flex justify-center mt-4 md:mt-0 md:justify-end items-center h-full">
                 <button id="predictButton" class="bg-indigo-600 hover:bg-indigo-700 text-white font-semibold py-2 px-5 rounded-md shadow-md focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-opacity-50 text-sm disabled:opacity-50 disabled:cursor-not-allowed" disabled>
                    Predict Manually
                 </button>
            </div>
        </div>

        <div id="cameraInputSection" class="hidden mb-6 flex flex-col items-center px-4">
             <label class="block text-xs font-medium text-gray-600 mb-2 uppercase tracking-wider">Camera Controls</label>
             <div class="flex flex-wrap justify-center gap-4">
                 <button id="startCameraButton" class="bg-green-600 hover:bg-green-700 text-white font-semibold py-2 px-5 rounded-md shadow-md text-sm disabled:opacity-50">Start Camera</button>
                 <button id="stopCameraButton" class="bg-red-600 hover:bg-red-700 text-white font-semibold py-2 px-5 rounded-md shadow-md text-sm disabled:opacity-50" disabled>Stop Camera</button>
                 <button id="switchCameraButton" class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-5 rounded-md shadow-md text-sm disabled:opacity-50" disabled>Switch Camera</button>
             </div>
             <video id="videoFeed" width="256" height="256" playsinline muted class="mt-4 hidden"></video>
             <p id="camErrorMsg" class="text-red-600 text-xs mt-2"></p>
        </div>

         <div class="mb-4 flex flex-col items-center border-t border-b border-indigo-200 py-3">
             <label for="alphaSlider" class="block text-xs font-medium text-gray-600 mb-2 uppercase tracking-wider">Overlay Alpha: <span id="alphaValue" class="font-bold text-indigo-600">0.40</span></label>
             <input type="range" id="alphaSlider" min="0" max="1" step="0.05" value="0.7" class="w-full max-w-md">
             <button id="togglePeaks" class="bg-green-600 hover:bg-green-700 text-white font-semibold py-2 px-5 rounded-md shadow-md text-sm disabled:opacity-50">Toggle Peaks</button>
        </div>

        <div class="text-center text-gray-600 text-xs mb-4 h-4">Live FPS: <span id="fpsDisplay">-</span><div id="messageArea" class="text-center text-purple-700 text-xs mb-4 h-4"></div></div>

        <div class="grid grid-cols-1 sm:grid-cols-2 gap-6">
            <div id="inputCanvasContainer" class="canvas-container">
                <canvas id="inputCanvas" width="256" height="256"></canvas>
                <label id="inputCanvasLabel" for="inputCanvas">Input</label>
            </div>
            <div class="canvas-container">
                <canvas id="outputCombined" width="256" height="256"></canvas>
                <label for="outputCombined">Combined Overlay</label>
            </div>
        </div>

        <div class="mt-8 pt-4 border-t border-indigo-200 text-center">
            <p class="text-xs text-gray-500">
                <a href="https://github.com/Elucidation/chessdetect-tfjs" target="_blank" rel="noopener noreferrer" class="text-indigo-600 hover:text-indigo-800 underline">Source Code on GitHub</a>
                <br>
                <a href="https://youtu.be/BVt12vzp_iM?si=TGQFCpzhNeoPmxRp" target="_blank" rel="noopener noreferrer" class="text-indigo-600 hover:text-indigo-800 underline">Video Explanation</a>
                <br>
                v1.05
            </p>
        </div>
    </div>

    <script>
        // --- DOM Elements ---
        const imageInput = document.getElementById('imageInput');
        const predictButton = document.getElementById('predictButton');
        const inputCanvas = document.getElementById('inputCanvas');
        const outputCombinedCanvas = document.getElementById('outputCombined');
        const messageArea = document.getElementById('messageArea');
        const modelStatus = document.getElementById('modelStatus');
        const alphaSlider = document.getElementById('alphaSlider');
        const alphaValueSpan = document.getElementById('alphaValue');
        const modeFileButton = document.getElementById('modeFileButton');
        const modeCameraButton = document.getElementById('modeCameraButton');
        const fileInputSection = document.getElementById('fileInputSection');
        const cameraInputSection = document.getElementById('cameraInputSection');
        const startCameraButton = document.getElementById('startCameraButton');
        const stopCameraButton = document.getElementById('stopCameraButton');
        const switchCameraButton = document.getElementById('switchCameraButton');
        const videoElement = document.getElementById('videoFeed');
        const camErrorMsg = document.getElementById('camErrorMsg');
        const inputCanvasContainer = document.getElementById('inputCanvasContainer');
        const inputCanvasLabel = document.getElementById('inputCanvasLabel');
        const togglePeaksButton = document.getElementById('togglePeaks');
        const fpsDisplay = document.getElementById('fpsDisplay');

        // Get 2D contexts
        const inputCtx = inputCanvas.getContext('2d');
        const outCombCtx = outputCombinedCanvas.getContext('2d'); // Still needed for drawing base image & overlays

        // --- Config ---
        const MODEL_URL = './tfjs_model_quantu8/model.json';
        const TARGET_IMG_SIZE = 128; // Model's expected input size
        const TARGET_FPS = 24;
        const MS_PER_FRAME = 1000 / TARGET_FPS;

        // --- State ---
        let loadedImage = null;
        let model = null;
        let lastPredictionTensor = null; // Store the last tensor for alpha adjustments
        let isPredicting = false;
        let isCameraMode = false;
        let isCameraRunning = false;
        let drawPeakOutlines = false;
        let videoStream = null;
        let animationFrameId = null;
        let lastPredictTime = 0;
        let lastPredictOnlyTime = 0; // Only time for last predict
        let frameCount = 0;
        let lastFpsUpdate = 0;
        let videoDevices = [];
        let currentDeviceId = null;
        let devicesEnumerated = false;

        // --- Temporary Canvases (Still needed as targets for tf.browser.toPixels) ---
        let tempCombinedCanvas = null;

        // --- Status Update Function ---
        function setStatus(text, showSpinner = false) { modelStatus.innerHTML = text + (showSpinner ? ' <div class="loader"></div>' : ''); }
        function setMessage(text = "") { messageArea.textContent = text; }
        function setCamError(text = "") { camErrorMsg.textContent = text; }

        // --- Mode Switching ---
        modeFileButton.addEventListener('click', () => switchMode('file'));
        modeCameraButton.addEventListener('click', () => switchMode('camera'));
        function switchMode(mode) {
             if (mode === 'file') {
                isCameraMode = false; stopCamera(); // Stop camera if switching away
                fileInputSection.classList.remove('hidden');
                cameraInputSection.classList.add('hidden');
                inputCanvasContainer.classList.remove('hidden'); // Show input canvas
                modeFileButton.classList.replace('bg-gray-300', 'bg-purple-600'); modeFileButton.classList.replace('text-gray-700', 'text-white');
                modeFileButton.classList.add('ring-2', 'ring-purple-600', 'z-10'); modeCameraButton.classList.replace('bg-purple-600', 'bg-gray-300');
                modeCameraButton.classList.replace('text-white', 'text-gray-700'); modeCameraButton.classList.remove('ring-2', 'ring-purple-600', 'z-10');
                resetUI(); // Reset UI elements for file mode
            } else { // camera mode
                isCameraMode = true;
                fileInputSection.classList.add('hidden');
                cameraInputSection.classList.remove('hidden');
                inputCanvasContainer.classList.add('hidden'); // Hide input canvas in camera mode
                modeCameraButton.classList.replace('bg-gray-300', 'bg-purple-600'); modeCameraButton.classList.replace('text-gray-700', 'text-white');
                modeCameraButton.classList.add('ring-2', 'ring-purple-600', 'z-10'); modeFileButton.classList.replace('bg-purple-600', 'bg-gray-300');
                modeFileButton.classList.replace('text-white', 'text-gray-700'); modeFileButton.classList.remove('ring-2', 'ring-purple-600', 'z-10');
                resetUI(); // Reset UI elements for camera mode
            }
        }

        // --- Camera Controls ---
        startCameraButton.addEventListener('click', () => startCamera());
        stopCameraButton.addEventListener('click', stopCamera);
        switchCameraButton.addEventListener('click', switchCamera);
        async function getCameraDevices() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices) { console.warn("enumerateDevices() is not supported."); return; }
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                videoDevices = devices.filter(device => device.kind === 'videoinput');
                console.log("Available video devices:", videoDevices);
                devicesEnumerated = true;
                switchCameraButton.disabled = !(videoDevices.length > 1 && isCameraRunning);
            } catch (err) { console.error("Error enumerating devices:", err); setCamError(`Error listing cameras: ${err.message}`); }
        }
        async function startCamera(deviceId = null) {
             if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) { setCamError("getUserMedia() not supported (HTTPS required on mobile)."); return; }
             if (!model) { setCamError("Model not loaded yet."); return; }
             if (isCameraRunning) return;
             setCamError(''); setMessage('Starting camera...'); startCameraButton.disabled = true; stopCameraButton.disabled = true; switchCameraButton.disabled = true;
             let constraints = { video: {} };
             if (deviceId) { constraints.video.deviceId = { exact: deviceId }; console.log("Attempting camera with deviceId:", deviceId); }
             else { constraints.video.facingMode = 'environment'; console.log("Attempting camera with facingMode: environment"); }
             try {
                 let stream = null;
                 try { stream = await navigator.mediaDevices.getUserMedia(constraints); }
                 catch (err) {
                      console.warn(`Failed getting camera with constraints: ${JSON.stringify(constraints)}. Error: ${err.name}`);
                      if (deviceId || constraints.video.facingMode === 'environment') {
                           console.log("Falling back to facingMode: user"); constraints.video = { facingMode: 'user' };
                           try { stream = await navigator.mediaDevices.getUserMedia(constraints); }
                           catch (err2) { console.warn(`Failed getting user camera. Falling back to any video.`); constraints.video = true; stream = await navigator.mediaDevices.getUserMedia(constraints); }
                      } else { throw err; }
                 }
                 videoStream = stream; videoElement.srcObject = videoStream;
                 const currentTrack = videoStream.getVideoTracks()[0];
                 if (currentTrack) { currentDeviceId = currentTrack.getSettings().deviceId; console.log("Using deviceId:", currentDeviceId); }
                 videoElement.onloadedmetadata = () => {
                     videoElement.play().then(async () => {
                         console.log("Video playback started."); isCameraRunning = true; stopCameraButton.disabled = false;
                         if (!devicesEnumerated) { await getCameraDevices(); }
                         else { switchCameraButton.disabled = !(videoDevices.length > 1 && isCameraRunning); } // Update button state
                         setMessage('Camera running. Starting predictions...');
                         lastPredictTime = performance.now(); frameCount = 0; lastFpsUpdate = lastPredictTime;
                         predictLoop(); // Start the prediction loop
                     }).catch(err => { console.error("Video play() failed:", err); setCamError(`Autoplay failed: ${err.message}.`); stopCameraButton.disabled = false; });
                 };
                  videoElement.onerror = (e) => { console.error("Video Element Error:", e); setCamError("Error playing video stream."); stopCamera(); }
             } catch (err) {
                 console.error("Error accessing camera:", err); setCamError(`Error accessing camera: ${err.name}`); setMessage('');
                 startCameraButton.disabled = !model; stopCameraButton.disabled = true; switchCameraButton.disabled = true;
             }
        }
        function stopCamera() {
            if (animationFrameId) { cancelAnimationFrame(animationFrameId); animationFrameId = null; }
            if (videoStream) { videoStream.getTracks().forEach(track => track.stop()); }
            videoElement.srcObject = null; isCameraRunning = false; currentDeviceId = null;
            startCameraButton.disabled = !model; stopCameraButton.disabled = true; switchCameraButton.disabled = true;
            setMessage('Camera stopped.'); clearOutputCanvases();
            // Clear input canvas only if in file mode (though it's hidden in camera mode anyway)
            if (!isCameraMode) inputCtx.clearRect(0, 0, inputCanvas.width, inputCanvas.height);
            fpsDisplay.textContent = '-';
        }
        async function switchCamera() {
            if (!isCameraRunning || videoDevices.length <= 1) { return; }
            console.log("Attempting to switch camera...");
            const currentIndex = videoDevices.findIndex(device => device.deviceId === currentDeviceId);
            const nextIndex = (currentIndex + 1) % videoDevices.length;
            const nextDeviceId = videoDevices[nextIndex].deviceId;
            console.log(`Switching from ${currentDeviceId} to ${nextDeviceId}`);
            stopCamera(); // Stop current stream first
            await new Promise(resolve => setTimeout(resolve, 100)); // Short delay might help hardware release
            await startCamera(nextDeviceId); // Start with the new device ID
        }

        // --- Model Loading ---
        async function loadAppModel() {
             setStatus('Loading model...', true); imageInput.disabled = true; predictButton.disabled = true; startCameraButton.disabled = true; switchCameraButton.disabled = true;
            try {
                await tf.setBackend('webgpu');
                model = await tf.loadGraphModel(MODEL_URL);
                setStatus('Warming up...', true);
                // Warmup with the correct input size
                const warmupTensor = tf.zeros([1, TARGET_IMG_SIZE, TARGET_IMG_SIZE, 3]);
                const warmupResult = model.predict(warmupTensor);
                // Dispose warmup results (handle potential array output)
                if (Array.isArray(warmupResult)) warmupResult.forEach(t => t.dispose()); else warmupResult.dispose();
                warmupTensor.dispose();
                setStatus('Model Ready'); console.log(`Graph Model ${MODEL_URL} loaded.`);
                // Enable controls now that model is ready
                imageInput.disabled = false;
                startCameraButton.disabled = false;
                // Predict button is enabled only when an image is loaded in file mode
            } catch (error) {
                console.error('Failed to load graph model:', error); setStatus(`Model Load Error!`);
                setMessage("Model loading failed. Check console/path.");
            }
        }

        // --- Image Loading & Display (File Mode) ---
        imageInput.addEventListener('change', async (event) => {
            if (isCameraMode) return; // Ignore if in camera mode
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = (e) => {
                    const img = new Image();
                    img.onload = async () => {
                        loadedImage = img;
                        drawInputImage(img); // Display the loaded image on the input canvas
                        clearOutputCanvases(); // Clear previous predictions
                        if (lastPredictionTensor) { // Dispose old tensor if a new image is loaded
                             tf.dispose(lastPredictionTensor);
                             lastPredictionTensor = null;
                        }
                        if (model && !isPredicting) {
                            setMessage('Image loaded. Auto-predicting...');
                            await runPrediction(); // Automatically predict on new image load
                        } else if (model) {
                            // Enable predict button only if model loaded but not auto-predicting (shouldn't happen often)
                            predictButton.disabled = false;
                            setMessage('Image loaded. Ready to predict.');
                        } else {
                            setMessage('Image loaded. Model not ready.');
                        }
                    };
                    img.onerror = () => { displayError("Error loading image."); }
                    img.src = e.target.result;
                };
                reader.onerror = () => { displayError("Error reading file."); }
                reader.readAsDataURL(file);
            } else {
                // If no file selected (e.g., user cancelled), reset relevant parts
                resetUI();
            }
        });

        // Draws the loaded image onto the input canvas, preserving aspect ratio
        function drawInputImage(img) {
            inputCtx.clearRect(0, 0, inputCanvas.width, inputCanvas.height);
            const aspectRatio = img.width / img.height;
            const targetAspectRatio = inputCanvas.width / inputCanvas.height; // Assuming square canvas
            let cropWidth = img.width; let cropHeight = img.height;
            // Calculate cropping dimensions to fit the target aspect ratio
            if (aspectRatio > targetAspectRatio) { // Image is wider than target
                cropWidth = img.height * targetAspectRatio;
            } else { // Image is taller than target
                cropHeight = img.width / targetAspectRatio;
            }
            const offsetX = (img.width - cropWidth) / 2;
            const offsetY = (img.height - cropHeight) / 2;
            // Draw the cropped portion of the image onto the canvas
            inputCtx.drawImage(img, offsetX, offsetY, cropWidth, cropHeight, 0, 0, inputCanvas.width, inputCanvas.height);
        }

        // --- Prediction Trigger (Manual Button - File Mode) ---
        predictButton.addEventListener('click', async () => {
             if (isCameraMode || !loadedImage || !model) {
                 displayError("Switch to File mode and load image first.");
                 return;
             }
             if (isPredicting) return; // Don't predict if already predicting
             await runPrediction();
        });


        // --- Core Prediction Logic (Refactored) ---
        async function runPrediction(sourceElement = null) {
            const currentSource = sourceElement || loadedImage;
            if (!currentSource || !model || isPredicting) {
                return; // Exit if no source, model, or already predicting
            }
            isPredicting = true; // Set prediction flag

            // Update UI for file mode prediction start
            if (!isCameraMode) {
                setStatus('Processing...', true);
                predictButton.disabled = true;
                imageInput.disabled = true; // Disable while predicting
                setMessage('Preprocessing & Predicting...');
                await new Promise(resolve => setTimeout(resolve, 10)); // Allow UI to update
            }

            let inputTensor = null;
            let outputTensor = null; // Keep the raw tensor output from the model

            try {
                // Preprocess the source image/video frame into a tensor
                inputTensor = preprocessSource(currentSource, TARGET_IMG_SIZE);

                const tStart = performance.now();

                // Run model prediction
                const predictionResult = tf.tidy(() => model.predict(inputTensor));

                // Handle potential array output from model and get the primary tensor
                outputTensor = Array.isArray(predictionResult) ? predictionResult[0] : predictionResult;

                // Squeeze the batch dimension if present (e.g., shape [1, H, W, C] -> [H, W, C])
                if (outputTensor.shape.length === 4 && outputTensor.shape[0] === 1) {
                    const squeezedTensor = outputTensor.squeeze([0]);
                    outputTensor.dispose(); // Dispose the original tensor with batch dim
                    outputTensor = squeezedTensor; // Use the squeezed tensor
                }
                // outputTensor should now have shape [H, W, C] (e.g., [128, 128, 5])

                const predictTime = performance.now() - tStart;
                lastPredictOnlyTime = predictTime;

                // Dispose the previously stored tensor before keeping the new one
                if (lastPredictionTensor) {
                    tf.dispose(lastPredictionTensor);
                    lastPredictionTensor = null;
                }
                // Keep a cloned tensor so it's not disposed by tf.tidy to be used by alpha slider
                lastPredictionTensor = tf.keep(outputTensor.clone());

                // Draw the combined output using the tensor
                await drawCombined(currentSource, outputTensor, parseFloat(alphaSlider.value), outputCombinedCanvas);

                if (!isCameraMode) {
                    setMessage(`Prediction took ${predictTime.toFixed(0)}ms`);
                }

            } catch (error) {
                console.error("Prediction Error:", error);
                if (!isCameraMode) displayError(`Processing error: ${error.message}`);
                // Ensure tensor is disposed on error
                if (lastPredictionTensor) {
                    tf.dispose(lastPredictionTensor);
                    lastPredictionTensor = null;
                }
            } finally {
                isPredicting = false; // Clear prediction flag
                // Update UI for file mode prediction end
                if (!isCameraMode) {
                    setStatus('Model Ready');
                    // Re-enable controls only if model and image are loaded
                    predictButton.disabled = !model || !loadedImage;
                    imageInput.disabled = !model;
                }
                // Dispose tensors used specifically in this run (input and the local outputTensor ref)
                // lastPredictionTensor is intentionally kept
                if (inputTensor) inputTensor.dispose();
                if (outputTensor) outputTensor.dispose();
            }
        }

        // --- Real-time Prediction Loop (Camera Mode) ---
        async function predictLoop() {
            if (!isCameraRunning || !model) { // Stop loop if camera off or model gone
                setMessage(model ? 'Camera stopped.' : 'Camera stopped, model unloaded.');
                fpsDisplay.textContent = '-';
                return;
             }

             const now = performance.now();
             const elapsed = now - lastPredictTime;

             // Update FPS counter approx every second
             frameCount++;
             if (now - lastFpsUpdate > 1000) {
                const timeTakenMs = now - lastFpsUpdate; // Time in ms.
                const perStepAvgMs = timeTakenMs / frameCount;
                const fps = frameCount / (timeTakenMs / 1000);
                // fpsDisplay.textContent = `${fps.toFixed(1)} | ${perStepAvgMs.toFixed(1)}ms/step | ${lastPredictOnlyTime.toFixed(1)}ms/predict`;
                fpsDisplay.textContent = fps.toFixed(1);
                frameCount = 0;
                lastFpsUpdate = now;
             }

             // Throttle predictions based on TARGET_FPS
             if (elapsed > MS_PER_FRAME && !isPredicting) {
                 lastPredictTime = now; // Update time *before* prediction starts
                 // Ensure video frame is ready before trying to predict
                 if (videoElement.readyState >= videoElement.HAVE_CURRENT_DATA) {
                      setMessage(" "); // Clear previous messages in camera mode
                      await runPrediction(videoElement); // Run prediction on the current video frame
                 }
             }

             // Request the next frame
             animationFrameId = requestAnimationFrame(predictLoop);
        }


        // --- Unified Preprocessing Function ---
        function preprocessSource(sourceElement, targetSize) {
             return tf.tidy(() => {
                 const srcWidth = sourceElement.videoWidth || sourceElement.width;
                 const srcHeight = sourceElement.videoHeight || sourceElement.height;

                 if (!srcWidth || !srcHeight) { throw new Error("Source element has invalid dimensions."); }

                 // Create tensor from pixels
                 let tensor = tf.browser.fromPixels(sourceElement).toFloat();

                 // Calculate cropping to maintain aspect ratio for square target
                 const aspectRatio = srcWidth / srcHeight;
                 const targetAspectRatio = 1.0; // targetSize / targetSize
                 let cropWidth = srcWidth; let cropHeight = srcHeight;
                 if (aspectRatio > targetAspectRatio) { // Wider than target
                     cropWidth = srcHeight * targetAspectRatio;
                 } else { // Taller than target
                     cropHeight = srcWidth / targetAspectRatio;
                 }
                 const offsetX = (srcWidth - cropWidth) / 2;
                 const offsetY = (srcHeight - cropHeight) / 2;

                 // Define the crop region relative to original dimensions [y1, x1, y2, x2]
                 const cropBox = [[
                    offsetY / srcHeight,
                    offsetX / srcWidth,
                    (offsetY + cropHeight) / srcHeight,
                    (offsetX + cropWidth) / srcWidth
                 ]];

                 // Crop and resize using tf.image.cropAndResize
                 // Input needs batch dimension, so expand dims
                 const cropped = tf.image.cropAndResize(
                     tensor.expandDims(0), // Add batch dim: [1, H, W, C]
                     cropBox,              // Boxes normalized coordinates
                     [0],                  // Box indices (only one box)
                     [targetSize, targetSize] // Target output size [height, width]
                 ); // Output shape: [1, targetSize, targetSize, 3]

                 // Normalize to 0-1 range
                 const normalized = cropped.div(tf.scalar(255.0));

                 // Dispose the initial tensor from pixels
                 tensor.dispose();

                 // Return the preprocessed tensor (still has batch dimension)
                 // Shape: [1, targetSize, targetSize, 3]
                 return normalized;
             });
        }


        // --- Visualization Function ---
        async function drawCombined(baseSourceElement, predictionTensor, alpha, targetCanvas) {
            if (!baseSourceElement || !predictionTensor) return;

            const targetCtx = targetCanvas.getContext('2d');
            const targetWidth = targetCanvas.width;
            const targetHeight = targetCanvas.height;

            const [predHeight, predWidth, channels] = predictionTensor.shape;
            if (channels !== 5) { console.error(`Prediction tensor has ${channels} channels, expected 5.`); return; }

            // --- Use only ONE temporary canvas ---
            if (!tempCombinedCanvas || tempCombinedCanvas.width !== predWidth || tempCombinedCanvas.height !== predHeight) {
                tempCombinedCanvas = document.createElement('canvas');
                tempCombinedCanvas.width = predWidth;
                tempCombinedCanvas.height = predHeight;
            }

            const heatmapColorsTensor = tf.tensor2d([[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]], [4, 3], 'float32');
            const peakMarkerColor = tf.tensor2d([[255, 0, 255]], [1, 3], 'int32'); // Magenta

            // --- Generate Combined Visualization Tensor ---
            // NOTE : Most of this is running on GPU via webgpu, avoid gpu<-> data transfer where possible.
            // This is the difference between 0.2ms and 20-30ms frame updates/
            const finalDrawTensor = tf.tidy(() => {
                // Calculate desired display alphas
                const segDisplayAlpha = Math.min(1.0, alpha + 0.2);

                // --- Segmentation Mask (Channel 0) ---
                const segChannel = tf.slice(predictionTensor, [0, 0, 0], [predHeight, predWidth, 1]);
                const segChannelScaled = segChannel.mul(175); // Segmentation as not fully white gray
                const segRgb = tf.concat([segChannelScaled, segChannelScaled, segChannelScaled], 2); // Shape [H, W, 3]

                // --- Heatmap (Channels 1-4) ---
                const heatmapChannels = tf.slice(predictionTensor, [0, 0, 1], [predHeight, predWidth, 4]); // Shape: [H, W, 4]
                const heatmapChannelsReshaped = heatmapChannels.reshape([predHeight * predWidth, 4]); // Shape: [H*W, 4]
                const heatmapCombined = tf.matMul(heatmapChannelsReshaped, heatmapColorsTensor);
                const heatmapRgb = heatmapCombined.reshape([predHeight, predWidth, 3]); // Shape [H, W, 3]

                // --- Blend Segmentation and Heatmap using Tensor Ops ---
                // Multiply each by their intended display alpha
                const segWeighted = segRgb.mul(segDisplayAlpha);
                const heatWeighted = heatmapRgb.mul(alpha);

                // Add them together (simple additive blending)
                // NOTE: This is different from overlay compositing!
                // Adjust blending logic here if needed (e.g., screen, multiply)
                const blendedRgb = segWeighted.add(heatWeighted);

                // Clip final values and convert to Int32 for toPixels
                const finalInt = blendedRgb.clipByValue(0, 255).toInt();

                if (drawPeakOutlines) {
                    // Get heatmap peaks - Find the argmax for each channel (axis=0)
                    const flatIndices = heatmapChannelsReshaped.argMax(0);
                    // Convert flat indices to [y, x] coordinates on GPU
                    const yCoords = tf.floor(flatIndices.div(TARGET_IMG_SIZE)).cast('int32'); // Shape [4] ~ y = Math.floor(flat / 128)
                    const xCoords = flatIndices.mod(TARGET_IMG_SIZE).cast('int32');           // Shape [4] ~ x = flat % 128
                    const xyCoords = tf.stack([yCoords, xCoords], 1);                         // Shape [4, 2] ~[[y,x],...] peaks

                    // Define offsets for the sprite shape used for peaks, a 5x5 hollow square in this case
                    const spriteOffsets = tf.tensor2d([
                        [-2, -2], [-2, -1], [-2, 0], [-2, 1], [-2, 2],
                        [-1, -2],                             [-1, 2],
                        [ 0, -2],                             [ 0, 2],
                        [ 1, -2],                             [ 1, 2],
                        [ 2, -2], [ 2, -1], [ 2, 0], [ 2, 1], [ 2, 2]
                    ], [16, 2], 'int32');

                    // Calculate all coordinates for the sprites using broadcasting
                    const allCoords = xyCoords.expandDims(1).add(spriteOffsets.expandDims(0));
                    const allCoordsFlat = allCoords.reshape([-1, 2]);

                    // Clip coordinates to stay within bounds [0, H-1] and [0, W-1]
                    const clippedCoords = tf.tidy(() => { // Tidy intermediate clipping tensors
                        const yClamped = allCoordsFlat.slice([0, 0], [-1, 1]).clipByValue(0, predHeight - 1);
                        const xClamped = allCoordsFlat.slice([0, 1], [-1, 1]).clipByValue(0, predWidth - 1);
                        return tf.concat([yClamped, xClamped], 1);
                    });

                    // Tile the color for each of the 20 points (4 peaks * 5 points/cross)
                    const peakUpdates = tf.tile(peakMarkerColor, [allCoordsFlat.shape[0], 1]); // Shape [20, 3]
                    
                    // Splat crosses on image tensor and return
                    return tf.tensorScatterUpdate(finalInt, clippedCoords, peakUpdates); // Shape [36, 3]
                }

                return finalInt; // Shape finalInt = [H, W, 3]
            });
            
            // NOTE: Key webgpu optimization, tf.browser.draw avoids gpu->cpu->gpu data transfer for drawing
            // const a = performance.now();
            const results = await tf.browser.draw(finalDrawTensor, tempCombinedCanvas);
            // console.info(`Took ${(performance.now() - a).toFixed(2)} ms to await draw`);

             // --- Dispose intermediate tensors ---
             finalDrawTensor.dispose();
             heatmapColorsTensor.dispose();

             // --- Draw Base Image ---
             targetCtx.clearRect(0, 0, targetWidth, targetHeight);
             targetCtx.imageSmoothingEnabled = true;
             const baseWidth = baseSourceElement.videoWidth || baseSourceElement.width;
             const baseHeight = baseSourceElement.videoHeight || baseSourceElement.height;
             if (!baseWidth || !baseHeight) { console.warn("Base source has no dimensions."); return; }
             const aspectRatio = baseWidth / baseHeight;
             const targetAspectRatio = targetWidth / targetHeight;
             let cropWidth = baseWidth; let cropHeight = baseHeight;
             if (aspectRatio > targetAspectRatio) { cropWidth = baseHeight * targetAspectRatio; } else { cropHeight = baseWidth / targetAspectRatio; }
             const offsetX = (baseWidth - cropWidth) / 2;
             const offsetY = (baseHeight - cropHeight) / 2;
             targetCtx.drawImage(baseSourceElement, offsetX, offsetY, cropWidth, cropHeight, 0, 0, targetWidth, targetHeight);

             // --- Draw SINGLE Overlay ---
             targetCtx.imageSmoothingEnabled = false; // Disable smoothing for pixelated overlay
             targetCtx.globalAlpha = alpha; // Draw the pre-blended overlay fully opaque
             targetCtx.drawImage(tempCombinedCanvas, 0, 0, predWidth, predHeight, 0, 0, targetWidth, targetHeight); // Scale prediction canvas to target size

             // Reset alpha
             targetCtx.globalAlpha = 1.0;
        }


        // --- Alpha Slider ---
        alphaSlider.addEventListener('input', async (event) => { // Added async
            const alpha = parseFloat(event.target.value);
            alphaValueSpan.textContent = alpha.toFixed(2); // Update displayed value

            const source = isCameraMode ? videoElement : loadedImage;

            // Use the stored tensor (lastPredictionTensor) to redraw immediately
            // This avoids re-running the model just for an alpha change
            if (source && lastPredictionTensor) {
                 // Avoid drawing if a prediction is currently running (esp. in camera mode)
                 // This prevents potential race conditions or drawing inconsistencies
                 if (!isPredicting) {
                    await drawCombined(source, lastPredictionTensor, alpha, outputCombinedCanvas);
                 }
            }
        });

        // --- Toggle Peaks Button ---
        togglePeaksButton.addEventListener('click', async () => {
            drawPeakOutlines = !drawPeakOutlines; // Toggle the state

            // Update button text/style for visual feedback (optional but recommended)
            if (drawPeakOutlines) {
                togglePeaksButton.textContent = 'Hide Peaks';
                togglePeaksButton.classList.replace('bg-green-600', 'bg-yellow-600');
                togglePeaksButton.classList.replace('hover:bg-green-700', 'hover:bg-yellow-700');
            } else {
                togglePeaksButton.textContent = 'Show Peaks';
                togglePeaksButton.classList.replace('bg-yellow-600', 'bg-green-600');
                togglePeaksButton.classList.replace('hover:bg-yellow-700', 'hover:bg-green-700');
            }

            // Redraw the output canvas if a prediction exists
            const source = isCameraMode ? videoElement : loadedImage;
            if (source && lastPredictionTensor && !isPredicting) {
                setMessage('Toggling peaks display...'); // Provide feedback
                await drawCombined(source, lastPredictionTensor, parseFloat(alphaSlider.value), outputCombinedCanvas);
                setMessage(''); // Clear feedback
            }
        });



        // Clears only the output visualization canvases
        function clearOutputCanvases() {
            outCombCtx.clearRect(0, 0, outputCombinedCanvas.width, outputCombinedCanvas.height);
        }

        // Reset UI elements and state, disposing the stored tensor
        function resetUI() {
            predictButton.disabled = true; // Disable manual predict initially
            loadedImage = null;

            // Dispose the stored prediction tensor if it exists
            if (lastPredictionTensor) {
                tf.dispose(lastPredictionTensor);
                lastPredictionTensor = null;
            }

            // Clear canvases based on mode
            clearOutputCanvases();
            if (!isCameraMode) {
                inputCtx.clearRect(0, 0, inputCanvas.width, inputCanvas.height);
                inputCanvasContainer.classList.remove('hidden'); // Ensure input canvas is visible in file mode
            } else {
                 inputCanvasContainer.classList.add('hidden'); // Ensure hidden in camera mode
            }

            setMessage(""); // Clear any messages
            alphaSlider.value = 0.7; // Reset slider
            alphaValueSpan.textContent = "0.7";
            imageInput.disabled = !model; // Disable file input if model not loaded
            imageInput.value = ''; // Clear file input selection

            // Reset toggle button text
            togglePeaksButton.textContent = 'Show Peaks'; // Add this line
            togglePeaksButton.classList.remove('bg-yellow-600', 'hover:bg-yellow-700'); // Ensure correct initial color
            togglePeaksButton.classList.add('bg-green-600', 'hover:bg-green-700'); // Ensure correct initial color

             // Reset camera button states appropriately
            if (model) { startCameraButton.disabled = false; }
            stopCameraButton.disabled = true;
            switchCameraButton.disabled = true;
        }

        // Display error message and reset state, disposing tensor
        function displayError(message) {
             setMessage(message);
             predictButton.disabled = true; // Disable predict on error
             loadedImage = null;
             // Dispose the stored prediction tensor on error
             if (lastPredictionTensor) {
                 tf.dispose(lastPredictionTensor);
                 lastPredictionTensor = null;
             }
             console.error(message);
             imageInput.disabled = !model; // Re-enable based on model status
        }

        // --- Load Model & Init ---
        loadAppModel();       // Start loading the model immediately
        switchMode('file'); // Start in file mode by default

    </script>

</body>
</html>